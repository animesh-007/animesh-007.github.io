<!DOCTYPE HTML>
<html lang="en"><head><meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
  <title>Animesh Gupta</title>
  
  <meta name="author" content="Animesh Gupta">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  
  <link rel="stylesheet" type="text/css" href="stylesheet.css">
  <link rel="icon" type="image/png" href="images/seal_icon.png">
  <style>
    #myimg{
      width:100%;
      max-width:100%;
      border-radius:50%;
      border: 1px solid #ddd;
  padding: 5px;
    }

    p {
      line-height: 22px;
      font-size: 15px;
    }

    ul li{
     font-size:15px;
    }
    
  </style>
</head>

<body>
  <table style="width:100%;max-width:800px;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
    <tr style="padding:0px">
      <td style="padding:0px">
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <tr style="padding:0px">
            <td style="padding:2.5%;width:63%;vertical-align:middle">
              <!-- <p style="text-align:center"> -->
                <!-- <name>Animesh Gupta</name> -->
                <p id="namechange" align="center">
                  <span id="a" style="color:#008000; font-size: 32px; font-weight: bold;">Animesh Gupta</span>
                  <span id="b" style="font-family: 'Gugi', cursive; font-size: 40px; color:#008000;">‡§Ö‡§®‡§ø‡§Æ‡•á‡§∑ ‡§ó‡•Å‡§™‡•ç‡§§‡§æ</span>

              </p>
              <p style="text-align:justify; font-size:16px; font-family:'Segoe UI', 'Helvetica Neue', sans-serif; line-height:1.7;">
                I am a PhD student at the <a href="https://www.ucf.edu/" style="color:#008000">University of Central Florida</a>, supervised by <a href="https://www.crcv.ucf.edu/person/mubarak-shah/" style="color:#008000">Dr. Mubarak Shah</a>. My research focuses on <strong>Multimodal Learning</strong> across <strong>images, videos, and 3D data</strong>, with a particular interest in developing models for <strong>Retrieval tasks</strong>.
              </p>

              <p style="text-align:justify; font-size:16px; font-family:'Segoe UI', 'Helvetica Neue', sans-serif; line-height:1.7;">
                Prior to this, I worked as a Machine Learning Engineer Intern at <a href="https://www.mvision.ai/" style="color:#008000">MVisionAI</a>, where I contributed to medical image registration for radiotherapy planning. I received my Bachelor's degree in Electronics and Computer Science from <a href="http://www.thapar.edu/" style="color:#008000">Thapar University, India</a>. I also completed research internships at <a href="https://en.uit.no/" style="color:#008000">UiT - The Arctic University of Norway</a>, where I worked on coreset-based data selection for efficient model training, and at the <a href="https://sketchx.eecs.qmul.ac.uk/" style="color:#008000">SketchX Lab, University of Surrey</a>, contributing to sketch-based visual understanding. 
              </p>

              <p style="text-align:center">
                <a href="mailto:animeshgupta.thapar@gmail.com" style="color:#008000">Email</a> &nbsp/&nbsp
                <a href="https://scholar.google.co.uk/citations?user=CZvlGXcAAAAJ" style="color:#008000">Google Scholar</a> &nbsp/&nbsp
                <a href="https://twitter.com/Animesh0072" style="color:#008000">Twitter</a> &nbsp/&nbsp
                <a href="https://github.com/animesh-007" style="color:#008000">Github</a> &nbsp/&nbsp
                <a href="https://animesh-007.github.io/Animesh_resume_may23.pdf" style="color:#008000">Resume/CV</a>
              </p>
            </td>
            <td style="padding:2.5%;width:40%;max-width:40%">
              <!-- <a href="images/animesh.png"><img id = "myimg" alt="profile photo" src="images/animesh.png" class="hoverZoomLink" style="border-radius: 50%; border:0px"></a> -->
               <a href="images/animesh.jpeg">
                <img id="myimg" alt="profile photo" src="images/animesh.jpeg" class="hoverZoomLink"
                    style="width: 300px; height: 300px; border-radius: 50%; object-fit: cover; border: 0px;">
              </a>
            </td>
          </tr>
        </tbody></table>

        <h2 style="font-family:'Segoe UI', sans-serif; color:#008000;">What's New üì£</h2>
       
        <div style="font-family:'Segoe UI', sans-serif; font-size:16px;">
          <div style="display: flex; align-items: start; margin-bottom: 10px;">
            <span style="width: 2em;">üîî</span>
            <div style="white-space: nowrap; overflow: hidden; text-overflow: ellipsis;">
              <strong>[Jun 2025]</strong> ‚Äî Our paper 
              <a href="https://arxiv.org/abs/2506.05274" style="color:#008000; text-decoration: none;">
                From Play to Replay: Composed Video Retrieval for Temporally Fine-Grained Videos
              </a> is live on arXiv
            </div>
          </div>
          <!-- </div> -->

          <div style="display: flex; align-items: start; margin-bottom: 10px;">
            <span style="width: 2em;">üéì</span>
            <div><strong>[Aug 2024]</strong> ‚Äî Started PhD at 
              <a href="https://www.ucf.edu/" style="color:#008000;">University of Central Florida</a>, supervised by 
              <a href="https://www.crcv.ucf.edu/person/mubarak-shah/" style="color:#008000;">Dr. Mubarak Shah</a>
            </div>
          </div>

          <div style="display: flex; align-items: start; margin-bottom: 10px;">
            <span style="width: 2em;">üìÑ</span>
            <div><strong>[May 2023]</strong> ‚Äî Paper 
              <a href="https://arxiv.org/pdf/2101.11606.pdf" style="color:#008000;">Big-Bench</a> accepted at TMLR 2023
            </div>
          </div>

          <div style="display: flex; align-items: start; margin-bottom: 10px;">
            <span style="width: 2em;">üíº</span>
            <div><strong>[Feb 2023]</strong> ‚Äî Started internship at 
              <a href="https://www.mvision.ai/" style="color:#008000;">MVisionAI</a>
            </div>
          </div>

          <div style="display: flex; align-items: start; margin-bottom: 10px;">
            <span style="width: 2em;">üñºÔ∏è</span>
            <div><strong>[July 2022]</strong> ‚Äî 
              <a href="https://arxiv.org/pdf/2207.01723.pdf" style="color:#008000;">Adaptive Fine-Grained Sketch-Based Image Retrieval</a> accepted at ECCV 2022
            </div>
          </div>

          <div style="display: flex; align-items: start; margin-bottom: 10px;">
            <span style="width: 2em;">üî¨</span>
            <div><strong>[May 2022]</strong> ‚Äî Research internship at 
              <a href="https://en.uit.no/" style="color:#008000;">UiT - The Arctic University of Norway</a>
            </div>
          </div>

          <div style="display: flex; align-items: start; margin-bottom: 10px;">
            <span style="width: 2em;">üíª</span>
            <div><strong>[Mar 2022]</strong> ‚Äî Research internship at 
              <a href="https://www.nvidia.com/en-in/" style="color:#008000;">NVIDIA, India</a>
            </div>
          </div>

          <div style="display: flex; align-items: start; margin-bottom: 10px;">
            <span style="width: 2em;">‚úèÔ∏è</span>
            <div><strong>[July 2021]</strong> ‚Äî Research internship at 
              <a href="https://sketchx.eecs.qmul.ac.uk/" style="color:#008000;">SketchX, University of Surrey</a>
            </div>
          </div>
        </div>


        <h1></h1>

        <script>
          function myFunction(pub_name) {
            var x = document.getElementById(pub_name);
            if (x.style.display === 'none') {
              x.style.display = 'block';
            } else {
              x.style.display = 'none';
            }
          }
        </script>

        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <tr>
          <td style="width:100%;vertical-align:middle">
            <!-- <heading>Research</heading> -->
            <h2 style="font-family:'Segoe UI', sans-serif; color:#008000;">Research Interests üß≠</h2>
            <p>
              I am broadly interested in retrieval tasks using multimodal models that integrate vision, language, and 3D data, with emphasis on modeling efficiency and fine-grained temporal reasoning.
            </p>
          </td>
        </tr>
      </tbody></table>

       <title>Research Papers</title>
  <style>
    body {
      font-family: 'Segoe UI', sans-serif;
    }
    .paper-block {
      width: 100%;
      max-width: 1000px;
      margin: 0 auto 40px auto;
      border-spacing: 0px;
    }
    .paper-img {
      width: 200px;
      border-radius: 4px;
    }
    .paper-title {
      font-size: 18px;
      font-weight: bold;
      color: #000;
      margin: 0;
    }
    .paper-meta {
      font-size: 15px;
      margin: 4px 0;
    }
    .paper-links {
      font-size: 15px;
      margin: 6px 0;
    }
    .paper-links a {
      color: #008000;
      text-decoration: none;
      margin-right: 12px;
    }
    .abstract, .bibtex {
      display: none;
      font-size: 14px;
      margin-top: 8px;
      text-align: justify;
    }
    .bibtex {
      font-family: Courier;
      background: #f5f5f5;
      padding: 10px;
    }
    pre {
      margin: 0;
    }
      .paper-meta a {
    color: #008000;
    text-decoration: none;
  }
  .paper-meta a:hover {
    text-decoration: underline;
  }
  </style>
</head>
<body>

<h2 style="font-family:'Segoe UI', sans-serif; color:#008000;">Publications üìë</h2>

<!-- Paper 1 -->
<table class="paper-block">
  <tr>
    <td style="width:200px; vertical-align:top;">
      <img src="images/teaser-tfcovr.png" class="paper-img">
    </td>
    <td style="padding-left:20px; vertical-align:top;">
      <p class="paper-title">From Play to Replay: Composed Video Retrieval for Temporally Fine-Grained Videos</p>
      <p class="paper-meta">
        <strong>Animesh Gupta</strong>, 
        <a href="#">Jay Parmar</a>, 
        <a href="#">Ishan Rajendrakumar Dave</a>, 
        <a href="#">Mubarak Shah</a><br>
        <em>ArXiv 2025</em>
      </p>
      <p class="paper-links">
        <a href="javascript:void(0);" onclick="toggleSection('abs4')">Abstract</a>
        <a href="https://github.com/UCF-CRCV/TF-CoVR" target="_blank">Code</a>
        <a href="https://arxiv.org/abs/2506.05274" target="_blank">Paper</a>
        <a href="javascript:void(0);" onclick="toggleSection('bib4')">BibTeX</a>
      </p>
      <div id="abs4" class="abstract">
        <em>Composed Video Retrieval (CoVR) retrieves a target video given a query video and a modification text describing the intended change. Existing CoVR benchmarks emphasize appearance shifts or coarse event changes and therefore do not test the ability to capture subtle, fast-paced temporal differences. We introduce TF-CoVR, the first large-scale benchmark dedicated to temporally fine-grained CoVR. TF-CoVR focuses on gymnastics and diving and provides 180K triplets drawn from FineGym and FineDiving. Previous CoVR benchmarks focusing on temporal aspect, link each query to a single target segment taken from the same video, limiting practical usefulness. In TF-CoVR, we instead construct each <query, modification> pair by prompting an LLM with the label differences between clips drawn from different videos; every pair is thus associated with multiple valid target videos (3.9 on average), reflecting real-world tasks such as sports-highlight generation. To model these temporal dynamics we propose TF-CoVR-Base, a concise two-stage training framework: (i) pre-train a video encoder on fine-grained action classification to obtain temporally discriminative embeddings; (ii) align the composed query with candidate videos using contrastive learning. We conduct the first comprehensive study of image, video, and general multimodal embedding (GME) models on temporally fine-grained composed retrieval in both zero-shot and fine-tuning regimes. On TF-CoVR, TF-CoVR-Base improves zero-shot mAP@50 from 5.92 (LanguageBind) to 7.51, and after fine-tuning raises the state-of-the-art from 19.83 to 25.82.</em>
      </div>
      <div id="bib4" class="bibtex">
<pre style="white-space: pre-wrap; word-break: break-word; margin: 0;">
@misc{gupta2025playreplaycomposedvideo,
      title={From Play to Replay: Composed Video Retrieval for Temporally Fine-Grained Videos}, 
      author={Animesh Gupta and Jay Parmar and Ishan Rajendrakumar Dave and Mubarak Shah},
      year={2025},
      eprint={2506.05274},
      archivePrefix={arXiv},
      primaryClass={cs.CV},
      url={https://arxiv.org/abs/2506.05274}, 
}
</pre>
      </div>
    </td>
  </tr>
</table>

<!-- Paper 2 -->
<table class="paper-block">
  <tr>
    <td style="width:200px; vertical-align:top;">
      <img src="images/data_efficient.png" class="paper-img">
    </td>
    <td style="padding-left:20px; vertical-align:top;">
      <p class="paper-title">Data-Efficient Training of CNNs and Transformers with Coresets: A Stability Perspective</p>
      <p class="paper-meta">
        <strong>Animesh Gupta</strong>, 
        <a href="#">Irtiza Hassan</a>, 
        <a href="#">Dilip K Prasad</a>, 
        <a href="#">Deepak K Gupta</a><br>
        <em>ArXiv 2024</em>
      </p>
      <p class="paper-links">
        <a href="javascript:void(0);" onclick="toggleSection('abs3')">Abstract</a>
        <a href="https://github.com/transmuteAI/Data-Efficient-Transformers" target="_blank">Code</a>
        <a href="https://arxiv.org/pdf/2303.02095" target="_blank">Paper</a>
        <a href="javascript:void(0);" onclick="toggleSection('bib3')">BibTeX</a>
      </p>
      <div id="abs3" class="abstract">
        <em>Coreset selection is among the most effective ways to reduce the training time of CNNs, however, only limited is known on how the resultant models will behave under variations of the coreset size, and choice of datasets and models. Moreover, given the recent paradigm shift towards transformer-based models, it is still an open question how coreset selection would impact their performance. There are several similar intriguing questions that need to be answered for a wide acceptance of coreset selection methods, and this paper attempts to answer some of these. We present a systematic benchmarking setup and perform a rigorous comparison of different coreset selection methods on CNNs and transformers. Our investigation reveals that under certain circumstances, random selection of subsets is more robust and stable when compared with the SOTA selection methods. We demonstrate that the conventional concept of uniform subset sampling across the various classes of the data is not the appropriate choice. Rather samples should be adaptively chosen based on the complexity of the data distribution for each class. Transformers are generally pretrained on large datasets, and we show that for certain target datasets, it helps to keep their performance stable at even very small coreset sizes. We further show that when no pretraining is done or when the pretrained transformer models are used with non-natural images (e.g. medical data), CNNs tend to generalize better than transformers at even very small coreset sizes. Lastly, we demonstrate that in the absence of the right pretraining, CNNs are better at learning the semantic coherence between spatially distant objects within an image, and these tend to outperform transformers at almost all choices of the coreset size.</em>
      </div>
      <div id="bib3" class="bibtex">
<pre style="white-space: pre-wrap; word-break: break-word; margin: 0;">
@article{gupta2023data,
  title={Data-Efficient Training of CNNs and Transformers with Coresets},
  author={Gupta, Animesh and Hassan, Irtiza and Prasad, Dilip K and Gupta, Deepak K},
  year={2023}
}
</pre>
      </div>
    </td>
  </tr>
</table>

<!-- Paper 3 -->
<table class="paper-block">
  <tr>
    <td style="width:200px; vertical-align:top;">
      <img src="images/big-bench.png" class="paper-img">
    </td>
    <td style="padding-left:20px; vertical-align:top;">
      <p class="paper-title">Beyond the Imitation Game: Quantifying and extrapolating the capabilities of language models</p>
      <p class="paper-meta">
        BIG-bench authors<br>
        <em>TMLR 2023</em>
      </p>
      <p class="paper-links">
        <a href="javascript:void(0);" onclick="toggleSection('abs2')">Abstract</a>
        <a href="https://github.com/google/BIG-bench" target="_blank">Code</a>
        <a href="https://arxiv.org/abs/2206.04615" target="_blank">Paper</a>
        <a href="javascript:void(0);" onclick="toggleSection('bib2')">BibTeX</a>
      </p>
      <div id="abs2" class="abstract">
        <em>Language models demonstrate both quantitative improvement and new qualitative capabilities with increasing scale. Despite their potentially transformative impact, these new capabilities are as yet poorly characterized. In order to inform future research, prepare for disruptive new model capabilities, and ameliorate socially harmful effects, it is vital that we understand the present and near-future capabilities and limitations of language models. To address this challenge, we introduce the Beyond the Imitation Game benchmark (BIG-bench). BIG-bench currently consists of 204 tasks, contributed by 450 authors across 132 institutions. Task topics are diverse, drawing problems from linguistics, childhood development, math, common-sense reasoning, biology, physics, social bias, software development, and beyond. BIG-bench focuses on tasks that are believed to be beyond the capabilities of current language models. We evaluate the behavior of OpenAI's GPT models, Google-internal dense transformer architectures, and Switch-style sparse transformers on BIG-bench, across model sizes spanning millions to hundreds of billions of parameters. In addition, a team of human expert raters performed all tasks in order to provide a strong baseline. Findings include: model performance and calibration both improve with scale, but are poor in absolute terms (and when compared with rater performance); performance is remarkably similar across model classes, though with benefits from sparsity; tasks that improve gradually and predictably commonly involve a large knowledge or memorization component, whereas tasks that exhibit "breakthrough" behavior at a critical scale often involve multiple steps or components, or brittle metrics; social bias typically increases with scale in settings with ambiguous context, but this can be improved with prompting.</em>
      </div>
      <div id="bib2" class="bibtex">
      <pre style="white-space: pre-wrap; word-break: break-word; margin: 0;">
      @article{srivastava2022beyond,
        title={Beyond the imitation game: Quantifying and extrapolating the capabilities of language models},
        author={Srivastava, Aarohi and Rastogi, Abhinav and Rao, Abhishek and Shoeb, Abu Awal Md and Abid, Abubakar and Fisch, Adam and Brown, Adam R and Santoro, Adam and Gupta, Aditya and Garriga-Alonso, Adri{\`a} and others},
        journal={arXiv preprint arXiv:2206.04615},
        year={2022}
      }
      </pre>
      </div>
    </td>
  </tr>
</table>

<!-- Paper 4 -->
<table class="paper-block">
  <tr>
    <td style="width:200px; vertical-align:top;">
      <img src="images/metafgsbir_main.png" class="paper-img">
    </td>
    <td style="padding-left:20px; vertical-align:top;">
      <p class="paper-title">Adaptive Fine-Grained Sketch-Based Image Retrieval</p>
      <p class="paper-meta">
        <a href="#">Ayan Kumar Bhunia</a>, <a href="#">Aneeshan Sain</a>, 
        <a href="#">Parth Hiren Shah</a>, <strong>Animesh Gupta</strong>, 
        <a href="#">Pinaki Nath Chowdhury</a>, <a href="#">Tao Xiang</a>, 
        <a href="#">Yi-Zhe Song</a><br>
        <em>ECCV 2022</em>
      </p>
      <p class="paper-links">
        <a href="javascript:void(0);" onclick="toggleSection('abs1')">Abstract</a>
        <a href="https://github.com/AyanKumarBhunia/Adaptive-FGSBIR" target="_blank">Code</a>
        <a href="https://arxiv.org/pdf/2207.01723.pdf" target="_blank">Paper</a>
        <a href="javascript:void(0);" onclick="toggleSection('bib1')">BibTeX</a>
      </p>
      <div id="abs1" class="abstract">
        <em>The recent focus on Fine-Grained Sketch-Based Image Retrieval (FG-SBIR) has shifted towards generalising a model to new categories without any training data from them. In real-world applications, however, a trained FG-SBIR model is often applied to both new categories and different human sketchers, i.e., different drawing styles. Although this complicates the generalisation problem, fortunately, a handful of examples are typically available, enabling the model to adapt to the new category/style. In this paper, we offer a novel perspective -- instead of asking for a model that generalises, we advocate for one that quickly adapts, with just very few samples during testing (in a few-shot manner). To solve this new problem, we introduce a novel model-agnostic meta-learning (MAML) based framework with several key modifications: (1) As a retrieval task with a margin-based contrastive loss, we simplify the MAML training in the inner loop to make it more stable and tractable. (2) The margin in our contrastive loss is also meta-learned with the rest of the model. (3) Three additional regularisation losses are introduced in the outer loop, to make the meta-learned FG-SBIR model more effective for category/style adaptation. Extensive experiments on public datasets suggest a large gain over generalisation and zero-shot based approaches, and a few strong few-shot baselines.</em>
      </div>
      <div id="bib1" class="bibtex">
<pre style="white-space: pre-wrap; word-break: break-word; margin: 0;">
@inproceedings{bhunia2022adaptive,
  title={Adaptive fine-grained sketch-based image retrieval},
  author={Bhunia, Ayan Kumar and Sain, Aneeshan and Shah, Parth Hiren and Gupta, Animesh and Chowdhury, Pinaki Nath and Xiang, Tao and Song, Yi-Zhe},
  booktitle={European Conference on Computer Vision},
  pages={163--181},
  year={2022},
  organization={Springer}
}
</pre>
      </div>
    </td>
  </tr>
</table>

<!-- JS: Toggle Function -->
<script>
  function toggleSection(id) {
    const el = document.getElementById(id);
    el.style.display = (el.style.display === 'none' || el.style.display === '') ? 'block' : 'none';
  }
</script>

<h2 style="font-family:'Segoe UI', sans-serif; color:#008000;">Research Experience üìö</h2>

<!-- Entry: MVisionAI -->
<div style="display: flex; gap: 20px; align-items: center; margin-bottom: 40px;">
  <div style="flex: 0 0 180px; text-align: center;">
  <img src="images/mvision.png" style="width: 100%; max-height: 140px; object-fit: contain;">
  </div>
  <div style="flex: 1;">
    <div style="font-weight: bold; font-size: 16px;">Machine Learning Engineer Intern, MVisionAI</div>
    <div style="font-style: italic; font-size: 14px;">February 2023 - Present</div>
    <div>Supervisor: Dr. Saad Ullah Akram</div>
    <ul style="margin-top: 8px;">
      <li>Worked on radiotherapy planning using multi-modal image registration (CT and MRI).</li>
      <li>Built an efficient library supporting multiple datasets and algorithms.</li>
      <li>Adapted RWCNet and Transmorph for OASIS and NLST datasets; established AbdomenCTCT baselines.</li>
    </ul>
  </div>
</div>

<!-- Entry: UiT -->
<div style="display: flex; gap: 20px; align-items: center; margin-bottom: 40px;">
  <div style="flex: 0 0 180px; text-align: center;">
    <img src="images/uit.png" style="width: 100%; max-height: 140px; object-fit: contain;">
  </div>
  <div style="flex: 1;">
    <div style="font-weight: bold; font-size: 16px;">Research Intern, University of Tromso</div>
    <div style="font-style: italic; font-size: 14px;">May 2022 - November 2022</div>
    <div>Supervisors: Dr. Deepak Gupta, Dr. Irtiza Hasan, Dr. Dilip Prasad</div>
    <ul style="margin-top: 8px;">
      <li>Created a benchmarking setup for coreset selection on CNNs and Transformers.</li>
      <li>Showed class-complexity-driven sampling outperforms uniform sampling.</li>
      <li>Work led to a research publication under review.</li>
    </ul>
  </div>
</div>

<!-- Entry: NVIDIA -->
<div style="display: flex; gap: 20px; align-items: center; margin-bottom: 40px;">
  <div style="flex: 0 0 180px; text-align: center;">
    <img src="images/nvidia.png" style="width: 100%; max-height: 140px; object-fit: contain;">
  </div>
  <div style="flex: 1;">
    <div style="font-weight: bold; font-size: 16px;">Research Intern, NVIDIA</div>
    <div style="font-style: italic; font-size: 14px;">March 2022 - May 2022</div>
    <ul style="margin-top: 8px;">
      <li>Worked on real-time lane detection and vision transformers for <a href="https://developer.nvidia.com/drive/drive-perception" style="color:#008000">DRIVE-Perceptron</a>.</li>
      <li>Focused on optimizing inference and improving real-world performance.</li>
    </ul>
  </div>
</div>

<!-- SketchX -->
<div style="display: flex; gap: 20px; align-items: center; margin-bottom: 40px;">
  <div style="flex: 0 0 180px; text-align: center;">
    <img src="images/sketchx.png" style="width: 100%; max-height: 140px; object-fit: contain;">
  </div>
  <div style="flex: 1;">
    <div style="font-weight: bold; font-size: 16px;">Research Intern, SketchX</div>
    <div style="font-style: italic; font-size: 14px;">July 2021 - March 2022</div>
    <div>Supervisor: Dr. Yi-Zhe Song</div>
    <ul style="margin-top: 8px;">
      <li>Worked on fine-grained and category-level sketch-based image retrieval.</li>
      <li>Co-authored ECCV 2022 paper on few-shot FG-SBIR adaptation across categories and styles.</li>
    </ul>
  </div>
</div>

<!-- GirlScript Summer of Code -->
<div style="display: flex; gap: 20px; align-items: center; margin-bottom: 40px;">
  <div style="flex: 0 0 180px; text-align: center;">
    <img src="images/gssoc.png" style="width: 100%; max-height: 140px; object-fit: contain;">
  </div>
  <div style="flex: 1;">
    <div style="font-weight: bold; font-size: 16px;">Intern, GirlScript Summer of Code</div>
    <div style="font-style: italic; font-size: 14px;">March 2021 - June 2021</div>
    <ul style="margin-top: 8px;">
      <li><a href="https://github.com/akshitagupta15june/Face-X" style="color:#008000">Face-X</a>: Implemented NasNet and Xception for face recognition. <a href="https://github.com/akshitagupta15june/Face-X/pulls?q=is%3Apr+author%3Aanimesh-007+is%3Aclosed" style="color:#008000">[PRs]</a></li>
      <li><a href="https://github.com/Comet-AI/Comet.Box" style="color:#008000">Comet.Box</a>: Added YOLOv5-based object detection example. <a href="https://github.com/Comet-AI/Comet.Box/pulls?q=is%3Apr+is%3Aclosed+author%3Aanimesh-007" style="color:#008000">[PRs]</a></li>
    </ul>
  </div>
</div>

<!-- Minus Zero -->
<div style="display: flex; gap: 20px; align-items: center; margin-bottom: 40px;">
  <div style="flex: 0 0 180px; text-align: center;">
    <img src="images/minuszero.png" style="width: 100%; max-height: 140px; object-fit: contain;">
  </div>
  <div style="flex: 1;">
    <div style="font-weight: bold; font-size: 16px;">Research Engineer, Minus Zero</div>
    <div style="font-style: italic; font-size: 14px;">October 2020 - March 2021</div>
    <ul style="margin-top: 8px;">
      <li>Worked on road segmentation for autonomous driving in Indian traffic scenarios.</li>
      <li>Trained FCHarDNet on a 10k-image dataset annotated with 34 semantic classes.</li>
    </ul>
  </div>
</div>

       
        <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
          <tr>
            <td>
            <br>
            <p align="right">
              <font size="2">
              <strong>I borrowed this website layout from <a target="_blank" href="https://jonbarron.info/">here</a>!</strong>
          </font>
            </p>
            </td>
          </tr>
          </table>
      </td>
    </tr>
  </table>
</body>

</html>
