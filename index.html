<!DOCTYPE HTML>
<html lang="en"><head><meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
  <title>Animesh Gupta</title>
  
  <meta name="author" content="Animesh Gupta">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  
  <link rel="stylesheet" type="text/css" href="stylesheet.css">
  <link rel="icon" type="image/png" href="images/seal_icon.png">
  <style>
    #myimg{
      width:100%;
      max-width:100%;
      border-radius:50%;
      border: 1px solid #ddd;
  padding: 5px;
    }

    p {
      line-height: 22px;
      font-size: 15px;
    }

    ul li{
     font-size:15px;
    }
    
  </style>
</head>

<body>
  <table style="width:100%;max-width:800px;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
    <tr style="padding:0px">
      <td style="padding:0px">
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <tr style="padding:0px">
            <td style="padding:2.5%;width:63%;vertical-align:middle">
              <!-- <p style="text-align:center"> -->
                <!-- <name>Animesh Gupta</name> -->
                <p id="namechange" align="center">
                  <span id="a"><name>Animesh Gupta</name></span><span id="b" style="font-family: 'Gugi', cursive; font-size: 40px;">अनिमेष गुप्ता </span>
              </p>
              <p style="text-align:justify" >
                I am currently working as Machine Learning Engineer Intern at <a href="https://www.mvision.ai/">MVisionAI</a>. I have completed my Bachelors in Electronics and Computer Science from <a href="http://www.thapar.edu/">Thapar University, India</a>. Previously, I worked as a Research Intern at <a href="https://en.uit.no/">UiT, Norway</a>, here I worked on project related to Coreset Subset Selection domain. I have also worked as a research intern at <a href="https://sketchx.eecs.qmul.ac.uk/">SketchX lab, Unviersity of Surrey, London</a>. At SketchX, I worked on research project dealing with Sketch Visual Understanding.

                <!-- I am a MASc student in University of Guelph, where I am advised by <a href="https://www.gwtaylor.ca">Prof. Graham Taylor</a>. I am also a student researcher at <a href="https://vectorinstitute.ai">Vector Institute</a>. 
                Previously, I was a Data Scientist at <a href="https://www.bayanat.ai"> Bayanat </a>, here I work on projects broadly related to Detection and Segmentation domains. 
                I have also worked as a research engineer at <a href="http://www.inceptioniai.org/">Inception Institute of Artificial Intelligence</a> working with <a href="https://sites.google.com/view/sanath-narayan">Dr Sanath Narayan</a> ,<a href="https://salman-h-khan.github.io/">Dr Salman Khan</a> and <a href="https://sites.google.com/view/fahadkhans/home">Dr Fahad Shahbaz Khan</a>. 
                At IIAI, I worked on research projects dealing with Open world and zero-shot object Detection, Generative Adversarial Networks and Few- and Zero- shot Learning problems. -->
                I also worked on industrial projects which try to solve Lane Detection, and Road Segmentation for autonomous cars.
              </p>
              <!-- <p style="text-align:justify" > 
                I was fortunate to spend a semester during my undergraduate studies at <a href="https://www.iitr.ac.in/">Indian Institute of Technology Roorkee</a>, 
                where I was supervised by <a href="https://scholar.google.co.in/citations?user=QU2O6JMAAAAJ&hl=en">Dr. Balasubramanian Raman</a>.
                Parrallel to my semester at IIT, I was selected as a outreachy intern, with <a href="https://www.mozilla.org/en-GB/">Mozilla</a> <a href="https://www.outreachy.org/alums/">(2018)</a>, where I was supervised by <a href="https://mozillians.org/en-US/u/emmairwin/">Emma Irwin</a>
              </p> -->
              <p style="text-align:center">
                <a href="mailto:animeshgupta.thapar@gmail.com">Email</a> &nbsp/&nbsp
                <a href="https://scholar.google.co.uk/citations?user=CZvlGXcAAAAJ">Google Scholar</a> &nbsp/&nbsp
                <a href="https://twitter.com/Animesh0072">Twitter</a> &nbsp/&nbsp
                <a href="https://github.com/animesh-007">Github</a> &nbsp/&nbsp
                <a href="https://animesh-007.github.io/Animesh_Gupta_Resume_Sep23.pdf">Resume/CV</a>
              </p>
            </td>
            <td style="padding:2.5%;width:40%;max-width:40%">
              <a href="images/animesh.png"><img id = "myimg" alt="profile photo" src="images/animesh.png" class="hoverZoomLink" style="border-radius: 70%; border:0px"></a>
            </td>
          </tr>
        </tbody></table>
        <table style="width:100%;border:0px;border-spacing:4px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <heading>What's New</heading>
          <br>
          <tr>              
              <td><strong>[May 2023]</strong></td>
              <td> Our paper <a href="https://arxiv.org/pdf/2101.11606.pdf"> Big-Bench </a> is accepted at TMLR 2023. </td>
          </tr>      
          <tr>              
              <td><strong>[Feb 2023]</strong></td>
              <td> Started interning at MVision AI </td>
          </tr> 
          <tr>              
            <td><strong>[July 2022]</strong></td>
            <td> <a href="https://arxiv.org/pdf/2207.01723.pdf">Adaptive Fine-Grained Sketch-Based Image Retrieval</a> is accpeted in the ECCV 2022. </td>
          </tr>            
          <tr>              
              <td><strong>[May 2022]</strong></td>
              <td> Joined as Research Intern at UiT - The Arctic University of Norway. </td>
          </tr>
          <tr>              
              <td><strong>[Mar 2022]</strong></td>
              <td> Joined as Research Intern at NVIDIA, India </td>
          </tr>
          <tr>              
              <td><strong>[July 2021]</strong></td>
              <td>Joined as Research Intern at SketchX, London </td>
          </tr>
          <tr>              
              <td><strong>[June 2021]</strong></td>
              <td>Recieved Grant by Weights & Biases for ML Reproducibility Challenge, Spring 2021. </td>
          </tr>
          <tr>              
              <td><strong>[Oct 2020]</strong></td>
              <td>Joined as Part-time Research Engineer at Minus Zero </td>
          </tr>
          <tr>              
              <td><strong>[Feb 2021]</strong></td>
              <td>Serving as a reviewer for ML Reproducibility Challenge 2020. </td>
          </tr>
        </tbody></table>

        <h1></h1>

        <script>
          function myFunction(pub_name) {
            var x = document.getElementById(pub_name);
            if (x.style.display === 'none') {
              x.style.display = 'block';
            } else {
              x.style.display = 'none';
            }
          }
        </script>

        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <tr>
          <td style="width:100%;vertical-align:middle">
            <heading>Research</heading>
            <p>
              I'm interested in developing methods which can effectively train neural networks with limited data. I'm also interested in developing models which can work Multi-Modalities setting like text and sketch. 
            </p>
          </td>
        </tr>
      </tbody></table>
      <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
        <tr>
            <!-- <td style="padding:20px;width:25%;vertical-align:middle"> -->
              <script type="text/javascript">
                function WACV2024_1_start() {
                  document.getElementById('WACV2024_1_image').style.opacity = "1";
                }

                function TMLR2023_1_stop() {
                  document.getElementById('WACV2024_1_image').style.opacity = "0";
                }
                WACV2024_1_stop()
              </script>
            <td style="vertical-align:middle">
               <!-- <div class="one"> -->
                <img src='images/data_efficient.png' width="200" style="vertical-align:middle; padding: auto;">
              <!-- </div> -->
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
            <a href="#">
                <papertitle>Data-Efficient Training of CNNs and Transformers with Coresets: A Stability Perspective</papertitle>
              </a>
              <br>
              <strong>Animesh Gupta</strong>,
              <a href="http://irtizahasan.com/">Irtiza Hassan</a>,
              <a href="https://sites.google.com/site/dilipprasad/home?authuser=0">Dilip K Prasad</a>,
              <a href="https://scholar.google.co.uk/citations?user=Nsxpe_kAAAAJ&hl=en">Deepak K Gupta,</a><br>
              <!-- <br> -->
<!--               (* denotes equal contribution)
 -->          <strong>In Submission WACV 2024 </strong>
              <br>
              <a href="javascript:void(0);" onclick="myFunction('WACV2024_1_abs')">
                <font size="3">Abstract</font>
              </a> /
              <a href="https://github.com/transmuteAI/Data-Efficient-Transformers" target="_blank">
                <font size="3">Code</font>
              </a> /
              <a href="https://arxiv.org/pdf/2303.02095" target="_blank" target="_blank">
                <font size="3">Paper</font>
              </a> /
              <a href="javascript:void(0);" onclick="myFunction('WACV2024_1_bib')">
                <font size="3">BibTex</font>
              </a>
              <p></p>
              <div id="WACV2024_1_abs" style="display:none; text-align:justify;min-width:350px;">
                <font size="3">
                  <em> Coreset selection is among the most effective ways to reduce the training time of CNNs, however, only limited is known on how the resultant models will behave under variations of the coreset size, and choice of datasets and models. Moreover, given the recent paradigm shift towards transformer-based models, it is still an open question how coreset selection would impact their performance. There are several similar intriguing questions that need to be answered for a wide acceptance of coreset selection methods, and this paper attempts to answer some of these. We present a systematic benchmarking setup and perform a rigorous comparison of different coreset selection methods on CNNs and transformers. Our investigation reveals that under certain circumstances, random selection of subsets is more robust and stable when compared with the SOTA selection methods. We demonstrate that the conventional concept of uniform subset sampling across the various classes of the data is not the appropriate choice. Rather samples should be adaptively chosen based on the complexity of the data distribution for each class. Transformers are generally pretrained on large datasets, and we show that for certain target datasets, it helps to keep their performance stable at even very small coreset sizes. We further show that when no pretraining is done or when the pretrained transformer models are used with non-natural images (e.g. medical data), CNNs tend to generalize better than transformers at even very small coreset sizes. Lastly, we demonstrate that in the absence of the right pretraining, CNNs are better at learning the semantic coherence between spatially distant objects within an image, and these tend to outperform transformers at almost all choices of the coreset size.</em>
                </font>
              </div>
              <div id="WACV2024_1_bib" style="font-family:Courier;display:none;min-width:350px;">
                <font size="2">
                  <br>
                  @article{gupta2023data,<br>
                    title={Data-Efficient Training of CNNs and Transformers with Coresets: A Stability Perspective},<br>
                    author={Gupta, Animesh and Hassan, Irtiza and Prasad, Dilip K and Gupta, Deepak K},<br>
                    journal={Paper preprint Paper:2303.02095},<br>
                    year={2023}<br>
                  }
                </font>
              </div>
            </td>
        </tr>
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
        <tr>
            <!-- <td style="padding:20px;width:25%;vertical-align:middle"> -->
              <script type="text/javascript">
                function TMLR2023_1_start() {
                  document.getElementById('TMLR2023_1_image').style.opacity = "1";
                }

                function TMLR2023_1_stop() {
                  document.getElementById('TMLR2023_1_image').style.opacity = "0";
                }
                TMLR2023_1_stop()
              </script>
            <td style="vertical-align:middle">
               <div class="one">
                <img src='images/big-bench.png' width="200" style="vertical-align:middle; padding: auto;">
              </div>
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
            <a href="#">
                <papertitle>Beyond the Imitation Game: Quantifying and extrapolating the capabilities of language models</papertitle>
              </a>
              <br>
              BIG-bench authors
              <br>
<!--               (* denotes equal contribution)
 -->          <strong>TMLR 2023 </strong>
              <br>
              <a href="javascript:void(0);" onclick="myFunction('TMLR2023_1_abs')">
                <font size="3">Abstract</font>
              </a> /
              <a href="https://github.com/google/BIG-bench" target="_blank">
                <font size="3">Code</font>
              </a> /
              <a href="https://openreview.net/pdf?id=uyTL5Bvosj" target="_blank" target="_blank">
                <font size="3">Paper</font>
              </a> /
              <a href="javascript:void(0);" onclick="myFunction('TMLR2023_1_bib')">
                <font size="3">BibTex</font>
              </a>
              <p></p>
              <div id="TMLR2023_1_abs" style="display:none; text-align:justify;min-width:350px;">
                <font size="3">
                  <em> Language models demonstrate both quantitative improvement and new qualitative capabilities with increasing scale. Despite their potentially transformative impact, these new capabilities are as yet poorly characterized. In order to inform future research, prepare for disruptive new model capabilities, and ameliorate socially harmful effects, it is vital that we understand the present and near-future capabilities and limitations of language models. To address this challenge, we introduce the Beyond the Imitation Game benchmark (BIG-bench). BIG-bench currently consists of 204 tasks, contributed by 450 authors across 132 institutions. Task topics are diverse, drawing problems from linguistics, childhood development, math, common-sense reasoning, biology, physics, social bias, software development, and beyond. BIG-bench focuses on tasks that are believed to be beyond the capabilities of current language models. We evaluate the behavior of OpenAI's GPT models, Google-internal dense transformer architectures, and Switch-style sparse transformers on BIG-bench, across model sizes spanning millions to hundreds of billions of parameters. In addition, a team of human expert raters performed all tasks in order to provide a strong baseline. Findings include: model performance and calibration both improve with scale, but are poor in absolute terms (and when compared with rater performance); performance is remarkably similar across model classes, though with benefits from sparsity; tasks that improve gradually and predictably commonly involve a large knowledge or memorization component, whereas tasks that exhibit "breakthrough" behavior at a critical scale often involve multiple steps or components, or brittle metrics; social bias typically increases with scale in settings with ambiguous context, but this can be improved with prompting.</em>
                </font>
              </div>
              <div id="TMLR2023_1_bib" style="font-family:Courier;display:none;min-width:350px;">
                <font size="2">
                  <br>
                  @article{srivastava2023beyond, <br>
                    title={Beyond the Imitation Game: Quantifying and extrapolating the capabilities of language models}, <br>
                    author={BIG-bench authors}, <br>
                    journal={Transactions on Machine Learning Research}, <br>
                    issn={2835-8856}, <br>
                    year={2023}, <br>
                    url={https://openreview.net/forum?id=uyTL5Bvosj}, <br>
                    note={}
                  }
                </font>
              </div>
            </td>
        </tr>
        </tbody></table>
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
        <tr>
          <script type="text/javascript">
            function ECCV2022_1_start() {
              document.getElementById('ECCV2022_1_image').style.opacity = "1";
            }

            function ECCV2022_1_stop() {
              document.getElementById('ECCV2022_1_image').style.opacity = "0";
            }
            ECCV2022_1_stop()
          </script>
          <td style="vertical-align:middle">
            <div class="one">
              <img src='images/metafgsbir_main.png' width="200" style="vertical-align:middle; padding: auto;">
            </div>
          </td>          
            <td style="padding:20px;width:75%;vertical-align:middle">
              <a href="#">
                <papertitle>Adaptive Fine-Grained Sketch-Based Image Retrieval</papertitle>
              </a>
              <br>
              <a href="https://ayankumarbhunia.github.io/">Ayan Kumar Bhunia</a>,
              <a href="https://aneeshan95.github.io/">Aneeshan Sain</a>,
              <a href="https://parthatom.github.io/">Parth Hiren Shah,</a><br>
              <strong>Animesh Gupta,</strong>
              <a href="http://www.pinakinathc.me/">Pinaki Nath Chowdhury,</a>
              <a href="https://www.surrey.ac.uk/people/tao-xiang">Tao Xiang,</a>
              <a href="http://personal.ee.surrey.ac.uk/Personal/Y.Song/">Yi-Zhe Song</a>
              <br>
              <strong> ECCV 2022 </strong>  
              <br>
              <a href="javascript:void(0);" onclick="myFunction('ECCV2022_1_abs')">
                <font size="3">Abstract</font>
              </a> /
              <a href="https://github.com/AyanKumarBhunia/Adaptive-FGSBIR" target="_blank">
                <font size="3">Code</font>
              </a> /
              <a href="https://arxiv.org/pdf/2207.01723.pdf" target="_blank" target="_blank">
                <font size="3">Paper</font>
              </a> /
              <a href="javascript:void(0);" onclick="myFunction('ECCV2022_1_bib')">
                <font size="3">BibTex</font>
              </a>
              <p></p>
              <div id="ECCV2022_1_abs" style="display:none; text-align:justify;min-width:350px;">
                <font size="3">
                  <em> The recent focus on Fine-Grained Sketch-Based Image Retrieval (FG-SBIR) has shifted towards generalising a model to new categories without any training data from them. In real-world applications, however, a trained FG-SBIR model is often applied to both new categories and different human sketchers, i.e., different drawing styles. Although this complicates the generalisation problem, fortunately, a handful of examples are typically available, enabling the model to adapt to the new category/style. In this paper, we offer a novel perspective -- instead of asking for a model that generalises, we advocate for one that quickly adapts, with just very few samples during testing (in a few-shot manner). To solve this new problem, we introduce a novel model-agnostic meta-learning (MAML) based framework with several key modifications: (1) As a retrieval task with a margin-based contrastive loss, we simplify the MAML training in the inner loop to make it more stable and tractable. (2) The margin in our contrastive loss is also meta-learned with the rest of the model. (3) Three additional regularisation losses are introduced in the outer loop, to make the meta-learned FG-SBIR model more effective for category/style adaptation. Extensive experiments on public datasets suggest a large gain over generalisation and zero-shot based approaches, and a few strong few-shot baselines.</em>
                </font>
              </div>
              <div id="ECCV2022_1_bib" style="font-family:Courier;display:none;min-width:350px;">
                <font size="2">
                  <br>
                  @InProceedings{adaptivefgsbir, <br>
                    author = {Ayan Kumar Bhunia and Aneeshan Sain and Parth Hiren Shah and Animesh Gupta and Pinaki Nath Chowdhury and Tao Xiang and Yi-Zhe Song}, <br>
                    title = {Adaptive Fine-Grained Sketch-Based Image Retrieval}, <br>
                    booktitle = {ECCV}, <br>
                    month = {October}, <br>
                    year = {2022}
                    }
                </font>
              </div>
            </td>
        </tr>
        </tbody></table>

        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
        <tr>
            <td width="100%" valign="middle">
              <heading>Research Experience</heading>
            </td>
          </tr>
        </table>
         <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <div class="one">
                <img src='images/mvision.png' width="150" style="padding:10px;vertical-align:middle">
              </div>
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
              <papertitle>Machine Learning Engineer Intern, MVisionAI </papertitle>
              <br>
              <em>February, 2023 - present</em>
              <br>
              Supervisors: Dr. Saad Ullah Akram
              <p>
                <ul>
                  <li>Working on easing treatment plan for radiotherapy using Image Registration. Radiotherapy involves multiple imaging modalities, e.g. full-field-of-view Computed Tomography (CT) scans is used for planning and Magnetic Resonance Imaging (MRI) scans is used for tumour segmentation.</li>
                  <li> Created an efficient library to facilitate multiple datasets and state-of-the-art algorithms.</li>
                  <li>Adapted RWCNet and Transmorph codebases to reproduce the results of the OASIS and NLST datasets. Formed baseline for the AbdomenCTCT and NLST datasets.</li>
                </ul>
                </p>
            </td>
        </tr>
        </tbody></table>
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
        <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <div class="one">
                <img src='images/uit.png' width="150" style="padding:10px;vertical-align:middle">
              </div>
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
              <papertitle>Research Intern, University of Tromso </papertitle>
              <br>
              <em>May 2022 - November 2022</em>
              <br>
              Supervisors: Dr. Deepak Gupta, Dr. Irtiza Hasan, and Dr. Dilip Prasad
              <p>
                <ul>
                  <li>Created a systematic benchmarking setup for different coreset methods on multiple CNNs and Transformers.</li>
                  <li> Demonstrated that the conventional concept of uniform subset sampling across the various classes of the data is not the appropriate choice</li>
                  <li>The findings of the internship led to a research publication, currently under review at a Machine Learning Journal.</li>
                </ul>
                </p>
            </td>
        </tr>
        </tbody></table>
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
        <tr>
            <td style="padding:20px;width:25%">
              <div class="one" style="height:auto;">
                <img src='images/nvidia.png' width="160" style="vertical-align:middle">
              </div>
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
              <papertitle>Research Intern, NVIDIA</papertitle>
              <br>
              <em>March 2022 - May 2022</em>
              <br>
              <!-- Supervisor: Emma Irwin -->
              <p> Experimented with latest Real-Time Lane Detection work and vision transformers for an improved solution for <a href="https://developer.nvidia.com/drive/drive-perception">DRIVE-Perceptron</a> platform with faster inference and performance.</p>
            </td>
        </tr>
        </tbody></table>
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
        <tr>
            <td style="padding:20px;width:25%">
              <div class="one" style="height:auto;">
                <img src='images/sketchx.png' width="160" style="vertical-align:middle">
              </div>
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
              <papertitle>Research Intern, SketchX</papertitle>
              <br>
              <em>July 2021 - March 2022</em>
              <br>
              Supervisor: Dr. Yi-Zhe Song
              <p>
                <ul>
                  <li>
                    Worked on Fine-Grained Sketch Based Image Retrieval and Category-Level Sketch Based Image Retrieval.
                  </li>
                  <li>
                    Contributed to the paper which created an adaptive Fine-Grained Sketch-Based Image Retrieval model. It adapts to new categories or different sketching patterns at test time, published in <strong>ECCV 2022</strong>
                  </li>
                </ul>
                 </p>

            </td>
        </tr>
        </tbody></table>
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <tr>
              <td style="padding:20px;width:25%">
                <div class="one" style="height:auto;">
                  <img src='images/gssoc.png' width="160" style="vertical-align:middle">
                </div>
              </td>
              <td style="padding:20px;width:75%;vertical-align:middle">
                <papertitle>Intern, GirlScript Summer of Code</papertitle>
                <br>
                <em>March 2021 - June 2021</em>
                <br>
                <!-- Supervisor: Dr. Yi-Zhe Song -->
                <p>
                  <ul>
                    <li>
                      <a href="https://github.com/akshitagupta15june/Face-X">Face-X</a>: Added NasNet and Xception model architecture for Face Recognition. <a href="https://github.com/akshitagupta15june/Face-X/pulls?q=is\%3Apr+author\%3Aanimesh-007+is\%3Aclosed">[PRs]</a>
                    </li>
                    <li>
                      <a href="https://github.com/Comet-AI/Comet.Box">Comet.Box</a>: Added YOLOv5 example for the object detection. <a href="https://github.com/Comet-AI/Comet.Box/pulls?q=is\%3Apr+is\%3Aclosed+author\%3Aanimesh-007">[PRs]</a>
                    </li>
                  </ul>
                   </p>
  
              </td>
          </tr>
          </tbody></table>
          <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
            <tr>
                <td style="padding:20px;width:25%">
                  <div class="one" style="height:auto;">
                    <img src='images/minuszero.png' width="160" style="vertical-align:middle">
                  </div>
                </td>
                <td style="padding:20px;width:75%;vertical-align:middle">
                  <papertitle>Research Engineer, Minus Zero</papertitle>
                  <br>
                  <em>October 2020 - March 2021</em>
                  <br>
                  <!-- Supervisor: Dr. Yi-Zhe Song -->
                  <p>
                    <ul>
                      <li>
                        Worked on the Road Segmentation problem for autonomous cars in India.
                      </li>
                      <li>
                        Used FCHarDNet as base architecture and trained on the Indian driving dataset (10k images and 34 classes).
                      </li>
                    </ul>
                     </p>
    
                </td>
            </tr>
            </tbody></table>
       
        <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
          <tr>
            <td>
            <br>
            <p align="right">
              <font size="2">
              <strong>I borrowed this website layout from <a target="_blank" href="https://jonbarron.info/">here</a>!</strong>
          </font>
            </p>
            </td>
          </tr>
          </table>
      </td>
    </tr>
  </table>
</body>

</html>
